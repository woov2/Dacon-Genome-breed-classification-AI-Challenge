{"cells":[{"cell_type":"markdown","metadata":{"id":"Hh4cNN69cMB_"},"source":["#  <center>SNP 정보를 활용한 유전체 품종 분류 모델링에 대한 연구 </center> \n","\n","## <center> 강민수, 이상우, 이상준</center>\n","\n","\n","\n","### <center> 국민대학교 </center>\n","### <center> AI빅데이터융합경영학과</center>\n","#### <center> daro980722@gmail.com, sangwoo710@naver.com, chukgoo11@naver.com</center>\n"]},{"cell_type":"markdown","metadata":{"id":"q_HJ44IifxuL"},"source":["## <center> 초록 </center>\n","\n","  유전체 염기서열에서 획득한 유전체 변이 정보인 Single Nucleotide Polymorphism 정보는 특정 개체 및 특정 품종에 따라 다른 변이 양상으로 나타난다. 즉 동일개체를 확인하거나, 동일 품종을 구분하는데 활용이 가능하다. 유전체 변이정보를 이용해서 정확히 유전체 품종을 분류해 내는 것은 농축수산 현장에서 품종의 다양성 혹은 품종 부정유통을 방지하는데 도움을 줄 수 있다.\n","본 논문에서는 유전체 품종 분류 문제를 해결하기 위하여 개체 정보와 SNP 정보를 이용하여 A,B,C 품종을 분류하는 최고의 품종구분 정확도를 획득하는 것을 목표로 데이터 기반 모델을 연구하였다. 분류 모델은 VotingClassifier를 활용하였으며, 오버피팅 문제를 감안하여 최소한의 파라미터 최적화를 진행하였고 SNP 정보를 활용해 Feature Engineering을 수행하였다. 연구에 활용한 데이터는 각 유전체 개체의 개체정보 및 15개의 SNP 정보로 구성되어 있으며, 약 300여개의 Train 데이터로 모델 학습 후 가장 높은 Macro F1 score를 갖는 모델들을 선별하여 이용하였다. 본 연구를 통해서 많은 SNP 정보를 통해 분류하는 것보다, 보다 더 적은 SNP 정보를 이용한 데이터 기반 분류 모델의 적용 가능성을 살펴 볼 수 있었다. "]},{"cell_type":"markdown","metadata":{"id":"b-Y1n7ZUeeIF"},"source":["# 1. 서론\n","\n","최근 시장에서 세 품종이 동시에 유통될 때, 각 품종의 고유한 생산푹목의 가치 및 가격 산정에 부정유통이 발생하고 있다. 이에 따라 부정유통이 차단되기 위해 현장에서 사용 할 수 있는 AI 모델에 대한 수요가 증가하고 있다. 정확한 품종 분류를 통해 품종의 다양성 혹은 품종 부정유통을 방지할 수 있고, 나아가 동일개체 확인, 동일 품종을 구분하는데 도움을 줄 수 있다.\n","\n","본 논문에서는 유전체 품종 분류 문제를 해결하기위해 데이터 기반의 분류 모델을 연구하였다. 유전체 개체 정보 및 SNP 정보 데이터를 바탕으로 Feature Engineering을 거친 데이터를 입력받아 유전체의 정확한 품종을 분류해내는 모델을 개발하였다.\n","연구에 활용된 데이터는 유전체의 개별 개체 정보 및 SNP 정보로 구성된 데이터로서, 보다 다양한 측면에서 SNP 정보를 활용하기 위하여 SNP 명, SNP 염색체 정보, SNP Genetic distance, SNP 각 마커의 유전체상 위치 정보 등을 참고한 변수들을 추가적으로 생성하여 활용하였다. \n","\n","모델 개발에 있어서는 다양한 sklearn에 내장된 모델들을 실험해 보았고, 결과적으로 단일 모델이 아닌 여러 모델들을 이용하는 것이 좋은 분류 성능을 낼 것으로 판단되어 BaggingClassifier, DecisionTreeClassifier, RidgeClassifier, XGBClassifier, LGBMClassifier, GradientBoostingClassifier, SVC, RidgeClassifierCV 및 RandomForestClassifier를 기반으로 하는 Hard voting 방식의 VotingClassifier를 최종 모델로 산정하였다. \n","\n","모델들의 성능을 실험할 때, Optuna를 이용해 하이퍼파라미터 튜닝을 하여 진행하였으나 데이터 셋의 크기 및 Feature들의 특성으로 인해 오버피팅이 발생하여 최종적으로는 random seed만 고정한 채 실험을 진행하였다. Sklearn에 내장된 대부분의 모델들을 전부 실험해 보았으며 그 결과 최종적으로 위 모델들이 가장 높은 Macro F1 score를 기록하여 사용하게 되었다. 양이 적고, 분류 클래스가 불균형인 데이터 셋의 특징에 초점을 두어 Data Oversampling, 여러 모델들을 이용한  VotingClassifier 그리고 최소한의 하이퍼파라미터 튜닝을 진행하여 최종 모델의 성능과 분류 정확도를 향상시킬 수 있었다."]},{"cell_type":"markdown","metadata":{"id":"kvFBFo7CXAvy"},"source":["# 2. 데이터 분석 및 처리"]},{"cell_type":"markdown","metadata":{"id":"X-IAc9JbrzwJ"},"source":["## 2.1 데이터 개요\n","\n","연구에 활용된 데이터는 유전체 정보 품종 분류 AI 경진대회의 일환으로 충남대학교, 티엔티리써치, AI Frenz에서 제공하는 유전체의 개별 개체 정보 및 SNP 데이터를 활용하였다. 연구 과정에서 약 300여개의 유전체 정보 데이터를 모델 학습을 위한 트레이닝 셋으로 사용하였고, 모델의 성능 평가를 위한 검증 셋을 따로 구축하지는 않았다."]},{"cell_type":"markdown","metadata":{"id":"rw2SpD-6IGZg"},"source":["\n","## 2.2 라이브러리 구성 및 데이터 로드\n","\n","데이터 분석 및 분류 모델 학습을 위하여 Sklearn, Imblearn, Pandas, Numpy, LightGBM, XGBoost등의 기본적인 라이브러리들을 활용하였으며, 추가적으로 Category_encoder를 이용하기 위해 category_encoders 라이브러리를 활용하였다.\n","CSV 형식으로 기록된 데이터셋을 데이터프레임 형식으로 로드하여, `train`(트레이닝셋), `test`(테스트셋) 변수로 활용하였다.\n","결과 재현을 위해 seed 고정을 추가적으로 수행하였다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1AbWdiRVPK9"},"outputs":[],"source":["#구글 드라이브 연결\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#코랩 환경 경로 설정 -> 자신에게 맞는 경로로 설정정\n","DATA_PATH = '/content/gdrive/MyDrive/유전체공모전/data/'\n","\n","# 코랩 기준 필요 라이브러리 설치\n","\n","# Category_encoders 설치\n","!pip install category_encoders\n","\n","#Base & visualization\n","import pandas as pd\n","import random\n","import os\n","import numpy as np\n","import warnings\n","import matplotlib.pylab as plt\n","import seaborn as sns\n","\n","#sklearn module & utils\n","from tqdm.notebook import tqdm\n","from sklearn.feature_selection import SelectPercentile\n","from sklearn.model_selection import StratifiedKFold , KFold, train_test_split, cross_val_score, cross_validate\n","warnings.filterwarnings('ignore') \n","\n","#Scaling\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","\n","# Encoding\n","import category_encoders as ce\n","\n","#Sampling\n","from imblearn.over_sampling import BorderlineSMOTE\n","\n","#Modeling\n","from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, VotingClassifier,RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import RidgeClassifier, RidgeClassifierCV\n","from sklearn.svm import SVC\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","\n","#Seed 고정\n","class CFG:\n","    SEED = 26\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","seed_everything(CFG.SEED) # Seed 고정\n","\n","train = pd.read_csv(DATA_PATH + 'train.csv')\n","test = pd.read_csv(DATA_PATH + 'test.csv')"]},{"cell_type":"markdown","metadata":{"id":"btH1H7KFZxCP"},"source":["`train` 및 `test`는 다음과 같은 형태로 구성되어 있다. 먼저 `train`에는 총 262개의 샘플이 존재하며, 각 샘플은 개체 정보 및 15개의 SNP 정보를 가지고 있다. \n","이와 대비하여 `test`는 총 175개의 샘플로 구성되어있으며, 두 데이터 모두 결측치는 존재하지 않는다. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1675377568438,"user":{"displayName":"강민수","userId":"02270149204876897928"},"user_tz":-540},"id":"wJfixQch7SHH","outputId":"67f9f9f3-fde7-497b-b189-4dada3c6874c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[95m<train 데이터 첫 5행 확인>\n","\u001b[0m\n","          id  father  mother  gender  trait SNP_01 SNP_02 SNP_03 SNP_04  \\\n","0  TRAIN_000       0       0       0      2    G G    A G    A A    G A   \n","1  TRAIN_001       0       0       0      2    A G    A G    C A    A A   \n","2  TRAIN_002       0       0       0      2    G G    G G    A A    G A   \n","3  TRAIN_003       0       0       0      1    A A    G G    A A    G A   \n","4  TRAIN_004       0       0       0      2    G G    G G    C C    A A   \n","\n","  SNP_05  ... SNP_07 SNP_08 SNP_09 SNP_10 SNP_11 SNP_12 SNP_13 SNP_14 SNP_15  \\\n","0    C A  ...    A A    G G    A A    G G    A G    A A    A A    A A    A A   \n","1    A A  ...    A A    G A    A A    A G    A A    G A    G G    A A    A A   \n","2    C C  ...    A A    G A    G A    A G    A A    A A    A A    A A    A A   \n","3    A A  ...    G G    A A    G G    A G    G G    G G    G G    A A    G G   \n","4    C C  ...    A A    A A    A A    G G    A A    A A    A G    A A    G A   \n","\n","  class  \n","0     B  \n","1     C  \n","2     B  \n","3     A  \n","4     C  \n","\n","[5 rows x 21 columns]\n","-----------------------------------------------------------------\n","\u001b[95m<train 데이터 마지막 5행 확인>\n","\u001b[0m\n","            id  father  mother  gender  trait SNP_01 SNP_02 SNP_03 SNP_04  \\\n","257  TRAIN_257       0       0       0      2    A G    A G    A A    G A   \n","258  TRAIN_258       0       0       0      2    G G    A A    C A    A A   \n","259  TRAIN_259       0       0       0      1    A G    G G    A A    G A   \n","260  TRAIN_260       0       0       0      1    A A    G G    A A    G A   \n","261  TRAIN_261       0       0       0      2    G G    A G    C A    G G   \n","\n","    SNP_05  ... SNP_07 SNP_08 SNP_09 SNP_10 SNP_11 SNP_12 SNP_13 SNP_14  \\\n","257    C C  ...    A A    G A    A A    G G    A G    G A    A A    A A   \n","258    A A  ...    G A    G A    A A    A G    A G    A A    A G    A A   \n","259    A A  ...    G G    G A    G A    A A    G G    G G    G G    C A   \n","260    A A  ...    G G    A A    G A    A G    A G    G A    G G    C A   \n","261    C C  ...    A A    A A    A A    G G    A A    A A    G G    A A   \n","\n","    SNP_15 class  \n","257    A A     B  \n","258    G A     C  \n","259    G G     A  \n","260    G G     A  \n","261    G A     B  \n","\n","[5 rows x 21 columns]\n","-----------------------------------------------------------------\n","\u001b[95m<train 데이터 정보>\n","\u001b[0m\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 262 entries, 0 to 261\n","Data columns (total 21 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   id      262 non-null    object\n"," 1   father  262 non-null    int64 \n"," 2   mother  262 non-null    int64 \n"," 3   gender  262 non-null    int64 \n"," 4   trait   262 non-null    int64 \n"," 5   SNP_01  262 non-null    object\n"," 6   SNP_02  262 non-null    object\n"," 7   SNP_03  262 non-null    object\n"," 8   SNP_04  262 non-null    object\n"," 9   SNP_05  262 non-null    object\n"," 10  SNP_06  262 non-null    object\n"," 11  SNP_07  262 non-null    object\n"," 12  SNP_08  262 non-null    object\n"," 13  SNP_09  262 non-null    object\n"," 14  SNP_10  262 non-null    object\n"," 15  SNP_11  262 non-null    object\n"," 16  SNP_12  262 non-null    object\n"," 17  SNP_13  262 non-null    object\n"," 18  SNP_14  262 non-null    object\n"," 19  SNP_15  262 non-null    object\n"," 20  class   262 non-null    object\n","dtypes: int64(4), object(17)\n","memory usage: 43.1+ KB\n","None\n","-----------------------------------------------------------------\n","\u001b[95m<train 데이터 크기>\n","\u001b[0m\n","(262, 21)\n","-----------------------------------------------------------------\n","-----------------------------------------------------------------\n","\u001b[95m<test 데이터 첫 5행 확인>\n","\u001b[0m\n","         id  father  mother  gender  trait SNP_01 SNP_02 SNP_03 SNP_04 SNP_05  \\\n","0  TEST_000       0       0       0      1    A G    G G    A A    G A    A A   \n","1  TEST_001       0       0       0      2    G G    A G    C C    G G    C C   \n","2  TEST_002       0       0       0      2    G G    A G    A A    A A    C A   \n","3  TEST_003       0       0       0      2    G G    A G    C A    A A    C C   \n","4  TEST_004       0       0       0      1    A A    G G    A A    G G    A A   \n","\n","  SNP_06 SNP_07 SNP_08 SNP_09 SNP_10 SNP_11 SNP_12 SNP_13 SNP_14 SNP_15  \n","0    A G    G G    G A    G A    A G    A G    G A    G G    C A    G A  \n","1    A A    A A    A A    A A    G G    A G    A A    A A    A A    A A  \n","2    A G    A A    A A    A A    A G    A A    G A    G G    A A    G G  \n","3    A A    A A    A A    A A    G G    A A    G A    A G    A A    A A  \n","4    G G    G G    A A    G G    A G    G G    G A    G G    A A    G G  \n","-----------------------------------------------------------------\n","\u001b[95m<test 데이터 마지막 5행 확인>\n","\u001b[0m\n","           id  father  mother  gender  trait SNP_01 SNP_02 SNP_03 SNP_04  \\\n","170  TEST_170       0       0       0      2    A G    G G    C C    A A   \n","171  TEST_171       0       0       0      2    G G    A A    A A    A A   \n","172  TEST_172       0       0       0      2    G G    A A    A A    A A   \n","173  TEST_173       0       0       0      2    A G    G G    C A    G A   \n","174  TEST_174       0       0       0      2    G G    G G    C C    G A   \n","\n","    SNP_05 SNP_06 SNP_07 SNP_08 SNP_09 SNP_10 SNP_11 SNP_12 SNP_13 SNP_14  \\\n","170    C A    A G    A A    G G    A A    G G    G G    A A    A A    A A   \n","171    C A    A G    A A    A A    A A    A G    A A    A A    A G    A A   \n","172    C A    A G    A A    A A    A A    G G    A G    A A    A G    A A   \n","173    C C    G G    A A    G A    A A    G G    A G    A A    A A    A A   \n","174    C A    A A    G A    G G    A A    G G    G G    A A    A A    A A   \n","\n","    SNP_15  \n","170    G A  \n","171    G A  \n","172    G G  \n","173    A A  \n","174    A A  \n","-----------------------------------------------------------------\n","\u001b[95m<test 데이터 정보>\n","\u001b[0m\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 175 entries, 0 to 174\n","Data columns (total 20 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   id      175 non-null    object\n"," 1   father  175 non-null    int64 \n"," 2   mother  175 non-null    int64 \n"," 3   gender  175 non-null    int64 \n"," 4   trait   175 non-null    int64 \n"," 5   SNP_01  175 non-null    object\n"," 6   SNP_02  175 non-null    object\n"," 7   SNP_03  175 non-null    object\n"," 8   SNP_04  175 non-null    object\n"," 9   SNP_05  175 non-null    object\n"," 10  SNP_06  175 non-null    object\n"," 11  SNP_07  175 non-null    object\n"," 12  SNP_08  175 non-null    object\n"," 13  SNP_09  175 non-null    object\n"," 14  SNP_10  175 non-null    object\n"," 15  SNP_11  175 non-null    object\n"," 16  SNP_12  175 non-null    object\n"," 17  SNP_13  175 non-null    object\n"," 18  SNP_14  175 non-null    object\n"," 19  SNP_15  175 non-null    object\n","dtypes: int64(4), object(16)\n","memory usage: 27.5+ KB\n","None\n","-----------------------------------------------------------------\n","\u001b[95m<test 데이터 크기>\n","\u001b[0m\n","(175, 20)\n"]}],"source":["#@title 데이터 기초 구성 예시\n","\n","print(\"\\033[95m\" + \"<train 데이터 첫 5행 확인>\")\n","print(\"\\033[0m\")\n","print(train.head())\n","print(\"-----------------------------------------------------------------\")\n","print(\"\\033[95m\" + \"<train 데이터 마지막 5행 확인>\")\n","print(\"\\033[0m\")\n","print(train.tail())\n","print(\"-----------------------------------------------------------------\")\n","print(\"\\033[95m\" + \"<train 데이터 정보>\")\n","print(\"\\033[0m\")\n","print(train.info())\n","print(\"-----------------------------------------------------------------\")\n","print(\"\\033[95m\" + \"<train 데이터 크기>\")\n","print(\"\\033[0m\")\n","print(train.shape)\n","print(\"-----------------------------------------------------------------\")\n","print(\"-----------------------------------------------------------------\")\n","print(\"\\033[95m\" + \"<test 데이터 첫 5행 확인>\")\n","print(\"\\033[0m\")\n","print(test.head())\n","print(\"-----------------------------------------------------------------\")\n","print(\"\\033[95m\" + \"<test 데이터 마지막 5행 확인>\")\n","print(\"\\033[0m\")\n","print(test.tail())\n","print(\"-----------------------------------------------------------------\")\n","print(\"\\033[95m\" + \"<test 데이터 정보>\")\n","print(\"\\033[0m\")\n","print(test.info())\n","print(\"-----------------------------------------------------------------\")\n","print(\"\\033[95m\" + \"<test 데이터 크기>\")\n","print(\"\\033[0m\")\n","print(test.shape)"]},{"cell_type":"markdown","metadata":{"id":"6Y_8oBC8-6zP"},"source":["## 2.3 데이터 분석 및 전처리"]},{"cell_type":"markdown","metadata":{"id":"B0Z8oFR4eB9N"},"source":["### 2.3.1  탐색적 자료 분석 (EDA)\n","\n","데이터의 클래스 분포를 확인하기 위하여 트레이닝 데이터 `train`에 대해 다음과 같이 살펴보았다. 먼저 X와 Y로 데이터를 분리한 뒤 분석에 필요없는 ID column을 제거해주었다. 그 후 train_x, train_y, test_x 변수에 저장해주었다. 데이터의 A,B,C Target Class 분포를 시각화하여 본 결과, 클래스의 불균형을 확인할 수 있다. 이를 통하여 추후에 클래스 불균형을 해소시키기 위한 Data Oversampling이 필요함을 알 수 있었다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MQQDXWRoIm5"},"outputs":[],"source":["#X Y 데이터분리 및 id column drop\n","def get_x_y(df):\n","    if 'class' in df.columns:\n","        df_x = df.drop(columns=['id', 'class'])\n","        df_y = df['class']\n","        return df_x, df_y\n","    else:\n","        df_x = df.drop(columns=['id'])\n","        return df_x\n","\n","train_x, train_y = get_x_y(train)\n","test_x = get_x_y(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":712,"status":"ok","timestamp":1675377569133,"user":{"displayName":"강민수","userId":"02270149204876897928"},"user_tz":-540},"id":"gx1TV9eWoLho","outputId":"e00fef16-1a0a-4a83-c964-ec19a052b094"},"outputs":[{"data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f45ad5f6d00>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["#class imbalanced -> Oversampling 적용\n","sns.countplot(train_y)"]},{"cell_type":"markdown","metadata":{"id":"jem5fjA6XrEb"},"source":["### 2.3.2  필요없는 Column 제거\n","\n","데이터 분석을 통해 모든 데이터에서 동일한 값을 가지는 필요없는 column이 존재함을 확인하였다. 필요없는 column이 존재하는 경우, 모델 학습에 있어서 부정적인 영향을 미치기 때문에 이를 처리하기 위하여 다음과 같이 해당 column을 단순 drop 하였다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inE82MTWXZ3G"},"outputs":[],"source":["#train_x 데이터를 확인해본 결과, 동일한 값을 가지는 columns 제거 -> 모델링에 좋지 않은 영향을 줄 것으로 판단 \n","train_x = train_x.drop(columns=['father','mother','gender'])\n","test_x = test_x.drop(columns=['father','mother','gender'])"]},{"cell_type":"markdown","metadata":{"id":"rDZ0t-XfbPFi"},"source":["# 3. 모델링을 위한 데이터셋 구축"]},{"cell_type":"markdown","metadata":{"id":"jso5-VHzYtZl"},"source":["## 3.1 성능 개선을 위한 Feature engineering\n","\n","기존 원본 데이터는 개체 정보와 15개의 SNP 정보만이 존재하므로 보다 정확한 품종 분류를 위해서 경진대회 주최측에서 제공한 SNP_info정보를 이용해 추가적인 파생변수를 만드는 것이 필요하다고 판단하였다.\n","파생변수 생성을 위해 활용한 SNP_info 정보는 다음과 같다.\n","\n","1. name(SNP 명) & chrom(염색체 정보)\n","2.  chrom(염색체 정보)\n","3.  SNP Total combination\n","4. cm(Genetic distance)\n","5. SNP G,C,A matehmatical info\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4P98nyVXZ3H"},"outputs":[],"source":["train_x['2_BTA'] = train_x['SNP_01']\n","train_x['chrom_6'] = train_x['SNP_02'] + '-'+ train_x['SNP_03'] + '-' + train_x['SNP_04'] + '-' + train_x['SNP_05'] + '-' + train_x['SNP_06'] + '-' + train_x['SNP_07'] + '-' + train_x['SNP_08'] + '-' + train_x['SNP_09']\n","train_x['6_ARS_Parent'] = train_x['SNP_02']\n","train_x['6_ARS_BFGL'] = train_x['SNP_03'] + '-' + train_x['SNP_04'] + '-' + train_x['SNP_09']\n","train_x['6_BOVINE'] = train_x['SNP_05'] + '-' + train_x['SNP_06'] + '-' + train_x['SNP_08']\n","train_x['6_HAPMAP'] = train_x['SNP_07']\n","train_x['7_BTB'] = train_x['SNP_10']\n","train_x['8_ARS'] = train_x['SNP_11']\n","train_x['chrom_9'] = train_x['SNP_12'] + '-' + train_x['SNP_13'] + '-' + train_x['SNP_14']\n","train_x['9_HAPMAP'] = train_x['SNP_12'] + '-' + train_x['SNP_14']\n","train_x['9_BTB'] = train_x['SNP_13']\n","train_x['10_BOVINE'] = train_x['SNP_15']\n","train_x['SNP_total'] = train_x['SNP_01'] + '-' + train_x['SNP_02'] + '-' + train_x['SNP_03'] + '-' + train_x['SNP_04'] + '-' + train_x['SNP_05'] + '-' + train_x['SNP_06'] + '-' + train_x['SNP_07'] + '-' + train_x['SNP_08'] + '-' + train_x['SNP_09'] + '-' + train_x['SNP_10'] + '-' + train_x['SNP_11'] + '-' + train_x['SNP_12'] + '-' + train_x['SNP_13'] + '-' + train_x['SNP_14'] + '-' + train_x['SNP_15'] "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNBTE3acsB5J"},"outputs":[],"source":["test_x['2_BTA'] = test_x['SNP_01']\n","test_x['chrom_6'] = test_x['SNP_02'] + '-'+ test_x['SNP_03'] + '-' + test_x['SNP_04'] + '-' + test_x['SNP_05'] + '-' + test_x['SNP_06'] + '-' + test_x['SNP_07'] + '-' + test_x['SNP_08'] + '-' + test_x['SNP_09']\n","test_x['6_ARS_Parent'] = test_x['SNP_02']\n","test_x['6_ARS_BFGL'] = test_x['SNP_03'] + '-' + test_x['SNP_04'] + '-' + test_x['SNP_09']\n","test_x['6_BOVINE'] = test_x['SNP_05'] + '-' + test_x['SNP_06'] + '-' + test_x['SNP_08']\n","test_x['6_HAPMAP'] = test_x['SNP_07']\n","test_x['7_BTB'] = test_x['SNP_10']\n","test_x['8_ARS'] = test_x['SNP_11']\n","test_x['chrom_9'] = test_x['SNP_12'] + '-' + test_x['SNP_13'] + '-' + test_x['SNP_14']\n","test_x['9_HAPMAP'] = test_x['SNP_12'] + '-' + test_x['SNP_14']\n","test_x['9_BTB'] = test_x['SNP_13']\n","test_x['10_BOVINE'] = test_x['SNP_15']\n","test_x['SNP_total'] = test_x['SNP_01'] + '-' + test_x['SNP_02'] + '-' + test_x['SNP_03'] + '-' + test_x['SNP_04'] + '-' + test_x['SNP_05'] + '-' + test_x['SNP_06'] + '-' + test_x['SNP_07'] + '-' + test_x['SNP_08'] + '-' + test_x['SNP_09'] + '-' + test_x['SNP_10'] + '-' + test_x['SNP_11'] + '-' + test_x['SNP_12'] + '-' + test_x['SNP_13'] + '-' + test_x['SNP_14'] + '-' + test_x['SNP_15']"]},{"cell_type":"markdown","metadata":{"id":"RSL_3n_Xs6II"},"source":["## 3.1.2 공백 제거\n","\n","SNP 정보에 불필요한 공백이 존재하여 공백을 제거해주었다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOPLF0a6sr4Z"},"outputs":[],"source":["def blank(data_tr, data_te, columns):\n","    for i in columns :\n","        data_tr[i] = data_tr[i].apply(lambda x : x.replace(\" \",\"\"))\n","        data_te[i] = data_te[i].apply(lambda x : x.replace(\" \",\"\"))\n","        \n","    return data_tr, data_te\n","\n","target_columns = train_x.iloc[:,16:].columns.to_list()\n","train_x.iloc[:,16:], test_x.iloc[:,16:] = blank(train_x.iloc[:,16:], test_x.iloc[:,16:], target_columns)"]},{"cell_type":"markdown","metadata":{"id":"Nwde8Pvwtddp"},"source":["## 추가 Feature engineering"]},{"cell_type":"markdown","metadata":{"id":"Q37XLUuL6BiM"},"source":["본 Feature는 아래 참고문헌을 활용해 생성하였다.\n","\n","참고문헌 : [0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ub4yDcpPtXZq"},"outputs":[],"source":["train_x['concat'] = train_x.iloc[:,1:16].sum(axis=1).apply(lambda x : x.replace(\" \",\"\"))\n","train_x['numGC'] = train_x['concat'].apply(lambda x : x.count('C')+x.count('G'))\n","train_x['numA'] = train_x['concat'].apply(lambda x : x.count('A'))\n","train_x['numGC^2'] = train_x['numGC']**2\n","train_x['sub'] = train_x['numGC'] - train_x['numA']\n","train_x['H'] = train_x['numGC']*3 + train_x['numA']*2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XZZe4ROtasZ"},"outputs":[],"source":["test_x['concat'] = test_x.iloc[:,1:16].sum(axis=1).apply(lambda x : x.replace(\" \",\"\"))\n","test_x['numGC'] = test_x['concat'].apply(lambda x : x.count('C')+x.count('G'))\n","test_x['numA'] = test_x['concat'].apply(lambda x : x.count('A'))\n","test_x['numGC^2'] = test_x['numGC']**2\n","test_x['sub'] = test_x['numGC'] - test_x['numA']\n","test_x['H'] = test_x['numGC']*3 + test_x['numA']*2"]},{"cell_type":"markdown","metadata":{"id":"WJT82bZHHSf5"},"source":["## 3.2 모델 학습을 위한 데이터 구성\n","\n","데이터 내의 trait 변수는 분석결과 numeric 변수가 아닌 categorical 변수로 취급해야 맞다고 판단하여 데이터 type 변환을 수행하였다.\n","\n","또한, numerical 변수들은 서로 다른 scale을 가질 시 모델링 시에 좋지 못한 영향을 줄 것으로 판단하여 scaling을 진행하기 전에 categorical 변수와 numerical 변수를 구분해주고 최종적으로 StandardScaler를 이용하여 scaling을 수행하였다. \n","\n","추가적으로, CatBoostClassifier 모델의 내부적인 encoding 방식을 활용하기 위해 catboost_encoder를 함수로 구현하여 데이터 셋에 적용시켜 주었다. 그 결과, 모델 성능에 많은 향상이 있었다.\n","\n","EDA를 통해 클래스 불균형이 존재함을 알아내었고 이를 해소시키기 위해 Oversampling 방식 중, 데이터 셋 내 각 데이터의 경계값을 기준으로 sampling을 해주는 BorderlineSMOTE 방식을 이용하였다.\n","\n","Target 변수인 class 변수는 Label-Encoding하여 변환된 값을 학습에 사용하였다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6s9P6UuMXZ3H"},"outputs":[],"source":["#@title 데이터 구성을 위한 코드\n","train_x['trait'] = train_x['trait'].astype('object')\n","test_x['trait'] = test_x['trait'].astype('object')\n","\n","cat_features = train_x.select_dtypes(include=['object']).columns.to_list()\n","num_features = train_x.select_dtypes(exclude=['object']).columns.to_list()\n","\n","scaler = StandardScaler()\n","train_x[num_features] = scaler.fit_transform(train_x[num_features])\n","test_x[num_features] = scaler.transform(test_x[num_features])\n","\n","def catboost_encoder_multiclass(X,X_t,y):\n","    y = y.astype(str)\n","    enc = ce.OneHotEncoder().fit(y)\n","    y_onehot = enc.transform(y)\n","    class_names = y_onehot.columns\n","    X_obj = X.select_dtypes('object')\n","    X_t_obj = X_t.select_dtypes('object')\n","    X = X.select_dtypes(exclude='object')\n","    X_t = X_t.select_dtypes(exclude='object') \n","    for class_ in class_names:\n","        enc = ce.CatBoostEncoder()\n","        enc.fit(X_obj,y_onehot[class_])\n","        temp = enc.transform(X_obj)\n","        temp_t = enc.transform(X_t_obj)\n","        temp.columns = [str(x)+'_'+str(class_) for x in temp.columns]\n","        temp_t.columns = [str(x)+'_'+str(class_) for x in temp_t.columns]\n","        X = pd.concat([X,temp],axis=1)\n","        X_t = pd.concat([X_t,temp_t],axis=1)\n","      \n","    return X, X_t\n","\n","train_x, test_x = catboost_encoder_multiclass(train_x,test_x,train_y)\n","\n","# Class 불균형 문제 해결\n","train_x,train_y = BorderlineSMOTE(random_state=CFG.SEED).fit_resample(train_x,train_y)\n","\n","class_le = LabelEncoder()\n","train_y = class_le.fit_transform(train_y)"]},{"cell_type":"markdown","metadata":{"id":"qvwylQV5ITMY"},"source":["# 4. 예측 모델 구성 및 실험"]},{"cell_type":"markdown","metadata":{"id":"7UQij2nNmDS9"},"source":["### 4.1.0 사용한 모델 종류\n","본 연구에서는 학습 시 아래와 같은 여러 알고리즘들을 기반으로 실험을 시행하였다. **<br/> - BaggingClassifier <br/> - DecisionTreeClassifier <br/> - RandomForestClassifier <br/> - GradientBoostingClassifier <br/> - XGBClassifier <br/> - LGBMClassifier <br/> - SVC <br/> - RidgeClassifier <br/> - RidgeClassifierCV**"]},{"cell_type":"markdown","metadata":{"id":"kEtGMqdMGUcH"},"source":["### 4.1.1 BaggingClassifier\n","\n","#### 배깅(Bagging (1), Bootstrap aggregating(2)) 모델 :  \n","\n","- 다양한 분류기를 만드는 한 가지 방법은 각기 다른 훈련 알고리즘을 사용하는 것입니다.\n","또 다른 방법은 같은 알고리즘을 사용하지만 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시키는 것입니다.\n","훈련 세트에서 중복을 허용하여 샘플링하는 방법을 배깅 bagging (bootstrap aggregating의 줄임말)이라 하며,\n","중복을 허용하지 않고 샘플링하는 방식을 페이스팅 pasting 이라고 합니다.\n","\n","- 학습 데이터 -> 각각의 학습 데이터 (랜덤 데이터 추출) -> 각각의 모델 -> 배깅 모델 (회귀: 평균값, 분류: 다수결 투표로 가장 많이 나온값) -> 예측값\n","과적합 방지  \n","\n","- 배깅과 페이스팅에서는 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있습니다. 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링 할 수 있습니다.\n","\n","(1) Bootstrap: Boot + strap 으로 긴 부츠의 뒷 부분에 달린 고리  \n","(2) aggregating: 모음\n","\n","참고문헌 : [1]"]},{"cell_type":"markdown","metadata":{"id":"rJVuzwRxkV40"},"source":["### 4.1.2 DecisionTreeClassifier\n","결정트리란 의사결정 나무를 만드는 것이다. 각 특성들이 노드가 되고, 중요한것을 상위에서 시작해 분기해 나가면서 그에 맞는 예측을 하는 모델이다. 가지가 뻗어나가는 트리 모양이어서 이름이 붙여졌다.\n","\n","![이미지](https://velog.velcdn.com/images%2Fdlskawns%2Fpost%2F434ed1db-c0cf-4f99-a7e0-853127bb87ff%2Fimage.png)\n","이미지 출처: https://www.youtube.com/watch?v=_L39rN6gz7Y\n","\n","스탯 퀘스트의 이미지를 이용해서 설명하자면, 맨 위 루트노드부터 정해진 질문에 대한 True or False 답을 내리고, 그 이후에도 추가적인 질문에 대한 답을 내려 최종적으로 의사 결정을 하도록 하여 예측하는 모델이다.\n","\n","- Decision Tree의 작동 원리, 장단점(특성 상호작용)  \n","특성들 간의 중요도를 파악해 해당 특성을 Root Node(최상위 노드)로 선정하고, 이후 나머지 특성들에 대해 분할 후 자식노드의 불순도를 계산한다. 마지막으로 각 속성에 대한 Imformation Gain 계산 후 Imformation Gain이 최대가 되는 분기조건을 찾아 분기한다. 이 과정을 모든 leaf 노드의 불순도가 0이 될 때까지 반복한다.\n","결정트리의 장점으로는 이해하고 해석하기 쉽다는 점,화이트 박스 모델을 사용하여 결과를 해석하는데 수월하다는 점을 대표적으로 들 수 있다다. \n","반면, 단점으로는 지나치게 복잡한 트리를 만들어 과적합에 빠질 가능성이 높다는 점,데이터의 작은 변화로 인해 완전히 다른 트리가 만들어 질 수 있고 이로 인해 트리가 불안정할 수 있다는 점이 있다.\n","\n","- Decision Tree 모델의 용어  \n","노드(Node): 각 특성(feature들 의미)  \n","엣지(Edge): 분기할 때의 선을 의미  \n","지니 불순도(Gini Impurity): 해당 feature에 대한 data를 통해 target을 예측할 때, 얼마나 target과 관련없는 답이 섞여있는지를 파악하는 척도  \n","엔트로피(Entropy): 열역학의 용어인데, 말하자면 무질서의 정도이다. 보통은 지니 불순도를 더 활용하는 것으로 보인다.\n","\n","참고문헌 : [2]"]},{"cell_type":"markdown","metadata":{"id":"9wE1jQLX54tG"},"source":["### 4.1.3 RandomForestClassifier\n","\n","- 기본 결정트리는 해당 데이터에 대해 맞춰서 분류를 진행한 것이기 때문에 과적합 현상이 자주 나타났다.\n","그에 따라 이를 개선하기 위해 2001년 앙상블 기법으로 고안된 것이 랜덤 포레스트이다.\n","- 훈련 과정에서 구성한 다수의 결정 트리들을 랜덤하게 학습시켜 분류 또는 회귀의 결과도출에 사용한다.\n","즉, 특정 특성을 선택하는 트리를 여러개 생성하여 이들을 기반으로 작업을 수행하는 것이다.\n","- 각각의 트리가 독립적으로 학습하기 때문에 학습과정을 병렬화할 수 있다.\n","- 일반적인 의사결정트리는 Tree Correlation이라고 하는 특정 feature 하나가 정답에 많은 영향을 주게되면 대부분의 결과치가 유사하게 나타나는 문제점이 있었다.\n","하지만 랜덤 포레스트에서는 그러한 문제를 해결했고, 파라미터의 개수가 적어 튜닝도 쉽다.\n","- 타깃 예측을 잘하며 각각이 구별되는 여러개의 트리를 만들기 위해 무작위성이 부여된다.\n","- 대표적인 '배깅' 모델이다. (cf. 배깅(Bagging)은 bootstrap aggregating의 줄임말이다.)\n","- 결정트리의 단점을 보완하고 장점은 그대로 가지고 있는 모델이어서 별다른 조정 없이도 괜찮을 결과를 만들어낸다.\n","랜덤하게 만들어지기 때문에 random_state를 고정해야 같은 결과를 볼 수 있다.\n","- 트리 개수가 많아질 수록 시간이 더 오래 걸린다.\n","\n","참고문헌 : [3]"]},{"cell_type":"markdown","metadata":{"id":"4-jGeRLS6EG-"},"source":["### 4.1.4 GradientBoostingClassifier\n","\n","그래디언트 부스팅 (Gradient Boosting, 의사 결정 나무의 앙상블)\n","- 의사 결정 나무 모델에 대한 부스팅 모델 + 이전 모델에 의한 가중치가 주어진 데이터를 훈련시키는 것이 아닌 이전 모델에 의한 오차를 다음 모델에 훈련 시킴 (데이터가 아닌 오차를 훈련 시킴)\n","\n","- 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가, 이전 예측기가 만든 잔여 오차 (residual error)에 새로운 예측기를 학습시키는 알고리즘\n","\n","특징\n","- greedy 알고리즘\n","- scale에 강한 모델\n","- 다양한 loss function을 지원(huber)\n","- 오버피팅에 약함\n","- 속도가 느림, 계산 리소스가 높음\n","\n","참고문헌 : [4]"]},{"cell_type":"markdown","metadata":{"id":"BPcM0bxkkWN-"},"source":["### 4.1.5 XGBClassifier\n","\n","- XGBoost는 최적화된 그레디언트 부스팅 구현이 가능한 파이썬 라이브러리이다. XGBoost는 빠른 속도, 확장성, 이식성이 특징이며 캐글 등 머신러닝 경연 대회에서 우승 후보들이 사용하는 도구로 성능이 아주 좋다.\n","\n","- 이전 모델이 과소적합한 샘플에 가중치를 줘서 다음 모델에서 예측 정확도를 높이는 방식으로 모델을 보완해가는 부스팅 기법을 사용한다.\n","\n","- GBM보다 빠르고 조기종료가 가능하며 과적합 방지가 가능하다. 또한 분류와 회귀 둘 다 사용이 가능하다. 사이킷런에서 제공하지 않기 때문에 따로 설치가 필요하다.\n","\n","참고문헌 : [5]"]},{"cell_type":"markdown","metadata":{"id":"C_oWTzF0kWUG"},"source":["### 4.1.6 LGBMClassifier\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FuCWi1%2Fbtq9jFAgt8u%2FX2iCxCZkMgQRPOy11jiwFk%2Fimg.png)  \n","이미지 출처: https://www.slideshare.net/GabrielCyprianoSaca/xgboost-lightgbm\n","- LightGBM은 트리 기준 분할이 아닌 리프 기준 분할 방식을 사용한다. 트리의 균형을 맞추지 않고 최대 손실 값을 갖는 리프 노드를 지속적으로 분할하면서 깊고 비대칭적인 트리를 생성한다. 이렇게 하면 트리 기준 분할 방식에 비해 예측 오류 손실을 최소화할 수 있다.\n","\n","장점\n","- 학습하는데 시간이 짧다. (통상 XGBoost 학습속도의 1.3~1.5배)\n","- 메모리 사용량이 상대적으로 적다.\n","- 대용량 데이터 처리 가능\n","\n","단점\n","- 적은 데이터셋(공식문서 기준 만건 이하)에서는 오버피팅 발생 가능\n","\n","참고문헌 : [6]"]},{"cell_type":"markdown","metadata":{"id":"qfnD4W9QktVC"},"source":["### 4.1.7 SVC\n","\n","SVM(Support Vector Machine, SVC(분류), SVR(회귀))\n","- 선형 분류, 비선형 분류, 회귀, 이상치 탐색에도 사용할 수 있는 다목적 머신 러닝 모델\n","- 클래스가 다른 데이터들을 가장 큰 마진(margin)으로 분리해내는 선 또는 면을 찾아내는 것\n","- 기본 아이디어 : 클래스 사이에 가장 폭 넓은 도로를 찾는 것\n","- 특히 복잡한 분류 문제에 잘 들어맞으며 작거나 중간 크기의 데이터셋에 적합 특성이 비슷하고 스케일이 비슷할 때 성능이 좋다\n","\n","참고문헌 : [7]\n"]},{"cell_type":"markdown","metadata":{"id":"Hh241zKt590S"},"source":["### 4.1.8 RidgeClassifier\n","\n","- 선형 모델(Linear model)의 예측력 혹은 설명력을 높이기 위해 여러 정규화(regularization) 방법들을 사용한다.\n","-  기본 선형모델을 사용하다 보면 overfitting이 발생할 수 있다. Overfitting된 경우 데이터에 매우 적합되어 극단적으로 오르락내리락하는 그래프가 생성되며, 이를 표현하는 선형 회귀의 계수 값이 매우 크게 나타난다. 이렇게 Variance가 큰 상황을 막기 위해, 계수 자체가 크면 페널티를 주는 수식을 추가한 것이 Ridge regression이다. \n","x**n 과 같이 차원이 큰 파라미터를 사람이 인위적으로 소거할 수도 있지만, Ridge regression은 오차를 최소화하는 함수에 페널티를 줌으로써 보다 부드럽게 계수를 선택하는 차이가 있다. Ridge regression은 기본 선형 모델을 regularize 하여 좀 더 좋은 performance를 내기 위해 시도해볼 수 있는 아주 기본적인 기법이다.\n","- Ridge regression 자체는 선형 모델이지만 같은 원리를 사용하여 분류 문제에도 적용할 수 있게 만든 알고리즘이 Ridge classifier이다.\n","\n","참고문헌 : [8]"]},{"cell_type":"markdown","metadata":{"id":"wO7T_FDakta2"},"source":["### 4.1.9 RidgeClassifierCV\n","\n","- 선형 모델(Linear model)의 예측력 혹은 설명력을 높이기 위해 여러 정규화(regularization) 방법들을 사용한다.\n","-  기본 선형모델을 사용하다 보면 overfitting이 발생할 수 있다. Overfitting된 경우 데이터에 매우 적합되어 극단적으로 오르락내리락하는 그래프가 생성되며, 이를 표현하는 선형 회귀의 계수 값이 매우 크게 나타난다. 이렇게 Variance가 큰 상황을 막기 위해, 계수 자체가 크면 페널티를 주는 수식을 추가한 것이 Ridge regression이다. \n","x**n 과 같이 차원이 큰 파라미터를 사람이 인위적으로 소거할 수도 있지만, Ridge regression은 오차를 최소화하는 함수에 페널티를 줌으로써 보다 부드럽게 계수를 선택하는 차이가 있다. Ridge regression은 기본 선형 모델을 regularize 하여 좀 더 좋은 performance를 내기 위해 시도해볼 수 있는 아주 기본적인 기법이다.\n","- Ridge regression 자체는 선형 모델이지만 같은 원리를 사용하여 분류 문제에도 적용할 수 있게 만든 알고리즘이 Ridge classifier이다.\n","- CV(cross validation)는 K-fold 교차 검증을 뜻하는 것인데, 이 원리는 간단히 말해 데이터를 train set과 test set으로 나누듯 지정된 k개로 나눈뒤 k번씩 자체적인 검증을 해 정확도를 올려준다. 데이터가 부족해 교차검증이 힘든경우에도 이를 통해 자체적인 교차검증을 할 수 있다. RidgeClassifier와는 다르게 교차 검증이 내장되어 있는 알고리즘이다.\n","\n","참고문헌 : [9]"]},{"cell_type":"markdown","metadata":{"id":"LPUnnHPof5Tq"},"source":["## 4.2 학습 및 성능평가\n"]},{"cell_type":"markdown","metadata":{"id":"C-FNQ9A38zVw"},"source":["### 4.2.0 VotingClassifier\n","\n","VotingClassifier는 \"다수결 분류\"를 뜻하는 것으로, 두 가지 방법으로 분류할 수 있습니다.\n","\n","1. Hard Voting Classifier\n","  - 여러 모델을 생성하고 그 성과(결과)를 비교합니다. 이 때 classifier의 결과들을 집계하여 가장 많은 표를 얻는 클래스를 \n","최종 예측값으로 정하는 것을 Hard Voting Classifier라고 합니다.\n","\n","2. Soft Voting Classifier\n","  - 앙상블에 사용되는 모든 분류기가 클래스의 확률을 예측할 수 있을 때 사용합니다.\n","각 분류기의 예측을 평균 내어 확률이 가장 높은 클래스로 예측하게 됩니다 (가중치 투표)\n","\n","참고문헌 : [10]"]},{"cell_type":"markdown","metadata":{"id":"-ql_ADE29T1N"},"source":["### 4.2.1 앙상블방식\n","- 성능향상을 위하여 앙상블(Ensemble) 기법을 사용하였다. 앙상블은 모델의 일반화 성능 향상을 위해 널리 활용되는 기법으로 다수의 예측 모델을 조합하여 최종 예측을 수행함으로써 예측 정확도를 향상시킬 수 있다.\n","- 앙상블을 위하여 VotingClassifier를 이용하여\n","  - BaggingClassifier\n","  - DecisionTreeClassifier\n","  - RandomForestClassifier\n","  - GradientBoostingClassifier\n","  - XGBClassifier\n","  - LGBMClassifier\n","  - SVC\n","  - RidgeClassifier\n","  - RidgeClassifierCV 들을 Hard Voting 방식으로 앙상블을 진행하였였습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2772,"status":"ok","timestamp":1675377572981,"user":{"displayName":"강민수","userId":"02270149204876897928"},"user_tz":-540},"id":"9QOlA8E38UW_","outputId":"bb695e6f-95de-4b1d-a8ce-f321addf4c63"},"outputs":[{"data":{"text/plain":["VotingClassifier(estimators=[('bag', BaggingClassifier(random_state=26)),\n","                             ('dt', DecisionTreeClassifier(random_state=26)),\n","                             ('rc', RidgeClassifier(random_state=26)),\n","                             ('xgb', XGBClassifier(random_state=26)),\n","                             ('lgb', LGBMClassifier(random_state=26)),\n","                             ('gb',\n","                              GradientBoostingClassifier(random_state=26)),\n","                             ('svc', SVC(random_state=26)),\n","                             ('rcc',\n","                              RidgeClassifierCV(alphas=array([ 0.1,  1. , 10. ]))),\n","                             ('rf', RandomForestClassifier(random_state=26))],\n","                 weights=[1, 1, 1, 2, 1, 1, 1, 1, 2])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["models = [\n","    ('bag', BaggingClassifier(random_state=CFG.SEED)),\n","    ('dt', DecisionTreeClassifier(random_state=CFG.SEED)),\n","    ('rc', RidgeClassifier(random_state=CFG.SEED)),\n","    ('xgb', XGBClassifier(random_state=CFG.SEED)),\n","    ('lgb', LGBMClassifier(random_state=CFG.SEED)),\n","    ('gb', GradientBoostingClassifier(random_state=CFG.SEED)),\n","    ('svc', SVC(random_state=CFG.SEED)),\n","    ('rcc', RidgeClassifierCV()),\n","    ('rf', RandomForestClassifier(random_state=CFG.SEED))\n","]\n","\n","best_model  = VotingClassifier(models, voting='hard', weights=[1,1,1,2,1,1,1,1,2])\n","best_model.fit(train_x,train_y)"]},{"cell_type":"markdown","metadata":{"id":"WJZyH5La-IN7"},"source":["### 4.2.2 Macro-F1\n","- Macro-F1점수는 클래스별/레이블별 F1-score의 평균으로 정의됩니다.\n","\n","Macro-F1의 특징\n","- Macro-F1 역시 0과 1사이의 값을 가지며 1에 가까울수록 좋습니다.\n","- Macro-F1의 경우 모든 class의 값에 동등한 중요성을 부여합니다. 즉, 비교적 적은 클래스(rare classes)에서 성능이 좋지 않다면, Macro-F1의 값은 낮게 나타날 것입니다.\n","- Macro-F1은  먼저 class와 label의 각각 F1-score를 계산한 뒤 평균내는 방식으로 작동합니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y3YRuwVy-paa"},"source":["### 4.2.3 모델별 성능평가\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6688,"status":"ok","timestamp":1675377579661,"user":{"displayName":"강민수","userId":"02270149204876897928"},"user_tz":-540},"id":"AaTcLOqbCQmk","outputId":"6dc49f5b-43fa-445c-dc0d-1323ceb1cfc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["BaggingClassifier 0.9563851225349838\n","DecisionTreeClassifier 0.9330261879890775\n","RidgeClassifier 0.9709130310194141\n","XGBClassifier 0.9651844688014901\n","LGBMClassifier 0.959105069231752\n","GradientBoostingClassifier 0.9650828507773443\n","SVC 0.9650224255917582\n","RidgeClassifierCV 0.9709130310194141\n","RandomForestClassifier 0.9650442825084091\n"]}],"source":["models = [\n","    (BaggingClassifier(random_state=CFG.SEED)),\n","    (DecisionTreeClassifier(random_state=CFG.SEED)),\n","    (RidgeClassifier(random_state=CFG.SEED)),\n","    (XGBClassifier(random_state=CFG.SEED)),\n","    (LGBMClassifier(random_state=CFG.SEED)),\n","    (GradientBoostingClassifier(random_state=CFG.SEED)),\n","    (SVC(random_state=CFG.SEED)),\n","    (RidgeClassifierCV()),\n","    (RandomForestClassifier(random_state=CFG.SEED))\n","]\n","models_name = [\n","    'BaggingClassifier',\n","    'DecisionTreeClassifier',\n","    'RidgeClassifier',\n","    'XGBClassifier',\n","    'LGBMClassifier',\n","    'GradientBoostingClassifier',\n","    'SVC',\n","    'RidgeClassifierCV',\n","    'RandomForestClassifier'\n","]\n","\n","cv_scores = []\n","for i in range(len(models)):\n","  cv_score = cross_val_score(models[i], train_x, train_y, scoring='f1_macro', cv=5).mean()\n","  cv_scores.append(cv_score)\n","  print(models_name[i], cv_score)"]},{"cell_type":"markdown","metadata":{"id":"GM2SJz9bCTaD"},"source":["|학습 모델|Macro-F1|\n","|------|:-----:|\n","|BaggingClassifier| 0.9563851225349838|\n","|DecisionTreeClassifier| 0.9330261879890775|\n","|RidgeClassifier| 0.9709130310194141|\n","|XGBClassifier| 0.9651844688014901|\n","|LGBMClassifier| 0.959105069231752|\n","|GradientBoostingClassifier| 0.9650828507773443|\n","|SVC| 0.9650224255917582|\n","|RidgeClassifierCV| 0.9709130310194141|\n","|RandomForestClassifier| 0.9650442825084091|"]},{"cell_type":"markdown","metadata":{"id":"USAMzqnlCvX4"},"source":["### 4.2.3 앙상블 후 성능평가"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9263,"status":"ok","timestamp":1675377613874,"user":{"displayName":"강민수","userId":"02270149204876897928"},"user_tz":-540},"id":"QQpKqwnHCwW4","outputId":"a489ef10-916d-4b9c-ba0b-4b7f346287f1"},"outputs":[{"data":{"text/plain":["0.9738376518811303"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["models = [\n","    ('bag', BaggingClassifier(random_state=CFG.SEED)),\n","    ('dt', DecisionTreeClassifier(random_state=CFG.SEED)),\n","    ('rc', RidgeClassifier(random_state=CFG.SEED)),\n","    ('xgb', XGBClassifier(random_state=CFG.SEED)),\n","    ('lgb', LGBMClassifier(random_state=CFG.SEED)),\n","    ('gb', GradientBoostingClassifier(random_state=CFG.SEED)),\n","    ('svc', SVC(random_state=CFG.SEED)),\n","    ('rcc', RidgeClassifierCV()),\n","    ('rf', RandomForestClassifier(random_state=CFG.SEED))\n","]\n","\n","best_model  = VotingClassifier(models, voting='hard', weights=[1,1,1,2,1,1,1,1,2])\n","best_model.fit(train_x,train_y)\n","\n","cv_score = cross_val_score(best_model, train_x, train_y, scoring='f1_macro', cv=5).mean()\n","cv_score"]},{"cell_type":"markdown","metadata":{"id":"BZbsybmNC1gY"},"source":["-  VotingClassifier로 Hard Voting ensemble 후 Macro-F1점수는 0.9738376518811303로 성능이 단일 모델들보다 월등히 오른 것을 확인할 수 있다.\n","\n","|학습 모델|Macro-F1|\n","|------|:-----:|\n","|VotingClassifier| 0.9738376518811303|"]},{"cell_type":"markdown","metadata":{"id":"NKTvSb_KV5T3"},"source":["## 4.3 결과 정리 및 분석"]},{"cell_type":"markdown","metadata":{"id":"AET5hGl6Xfb-"},"source":["#### 아래의 표 1과 같이 개별 모델에서 가장 성능이 좋았던 건 RidgeClassifier, RidgeClassifierCV로 비교적 최신 모델인 XGB, LGBM 알고리즘 보다 Macro-F1 socre가 좋은 것을 알 수 있다.\n","\n","#### 이러한 분석 결과를 바탕으로 다양한 계열의 알고리즘을 사용 및 앙상블을 진행 하였고 결과적으로 총 9개의 모델을 선택하였다.\n","  \n","#### 앙상블 이후의 경우에서는 단일 모델 9개보다 압도적인 성능을 보였으며 이는 단일 모델로는 높은 Macro-F1 score를 얻기 힘들다는 것을 방증한다.\n","\n","#### 이번 연구에서는 학습할 수 있는 데이터 셋이 비교적 작아서 단일 모델로는 좋은 결과를 얻기 힘들었던 것으로 판단된다. 다양한 계열의 알고리즘으로 도출된 결과가 이질적인 부분이 있었는데 이러한 이질성들을 HardVoting 방식으로 오분류를 줄여준 것이 모델의 정확성을 올려준 것으로 판단된다.\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ntLyz-XqNDla"},"source":["표 1. 학습 모델에 따른 Macro-F1 score\n","\n","|학습 모델|Macro-F1|\n","|------|:-----:|\n","|BaggingClassifier| 0.9563851225349838|\n","|DecisionTreeClassifier| 0.9330261879890775|\n","|RidgeClassifier| 0.9709130310194141|\n","|XGBClassifier| 0.9651844688014901|\n","|LGBMClassifier| 0.959105069231752|\n","|GradientBoostingClassifier| 0.9650828507773443|\n","|SVC| 0.9650224255917582|\n","|RidgeClassifierCV| 0.9709130310194141|\n","|RandomForestClassifier| 0.9650442825084091|\n","|VotingClassifier| 0.9738376518811303|"]},{"cell_type":"markdown","metadata":{"id":"MzgdPd6_S-nz"},"source":["# 5. 결론\n","\n","&nbsp;본 논문에서는 SNP정보 데이터를 바탕으로 유전체 품종을 분류하는 데이터 기반의 모델을 연구하였다.  \n","개발된 모델은 여러가지 알고리즘을 활용하였고 해당 모델들의 정확도와 성능을 향상시키기 위하여 VotingClassifier를 활용한 Hard Voting 앙상블(ensenble)기법을 활용하였다.  \n","&nbsp;앙상블 결과 개별 모델을 학습한 결과보다 가장 높은 Macro-F1 (0.9738376518811303)를 얻을 수 있었다.  \n","본 연구를 통해 최근에 가장 많이 사용하는 LGBM, CatBoost,XGB 뿐만아니라 분류 문제에 Bagging 및 Tree 계열 모델들의 활용 가능성을 알아볼 수 있었고 개발된 품종 분류 모델을 통해 유전체 품종 분류에 도움이 될 수 있을 것으로 생각한다."]},{"cell_type":"markdown","metadata":{"id":"RnYmzquzfTw9"},"source":["# 6. 참고문헌\n","[0] 위키디피아[웹사이트]. (2023.02.04). URL : https://ko.wikipedia.org/wiki/GC_%ED%95%A8%EB%9F%89\n","\n","[1] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n","\n","[2] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier\n","\n","[3] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n","\n","[4] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn-ensemble-gradientboostingclassifier\n","\n","[5] XGBoost[웹사이트]. (2023.02.04). URL: https://xgboost.readthedocs.io/en/stable/parameter.html\n","\n","[6] lightgbm[웹사이트]. (2023.02.04). URL: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n","\n","[7] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","\n","[8] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier\n","\n","[9] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifierCV.html#sklearn.linear_model.RidgeClassifierCV\n","\n","[10] scikit learn[웹사이트]. (2023.02.04). URL: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["q_HJ44IifxuL","b-Y1n7ZUeeIF","X-IAc9JbrzwJ","kEtGMqdMGUcH"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}